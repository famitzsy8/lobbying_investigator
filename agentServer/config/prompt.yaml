selector_prompt:
  description: >
    This is the last message in the conversation: 
    """
    {last_message}
    """

    You are given the following agents: {agent_names}
    If the last message corresponds to a specialist returning his findings, return "orchestrator"
    If the last message corresponds to the orchestrator asking all of the agents for confirmation return a random agent name.
    In all other cases reason from the content of the last message, which of the agents should be called.
    Return SOLELY thename of the agent that should be called next. (i.e. "committee_specialist")

planning_prompt:
  description: >
    You have attempted to call a tool without first providing a plan. 
    You MUST first provide a step-by-step plan as a bulleted list. 
    After the text of your plan, you MUST re-issue the same tool call you just attempted, and WITH the arguments according to the tool description and the plan. 
    Your final response must include BOTH the textual plan AND the `tool_calls` data structure.

arguments_prompt:
  description: >
    {warning}
    Our agent wants to call the following tool of our Model Context Protocol Server: {tool_name}
    For this he provided the following arguments: {sent_arguments}
    We need to verify if these arguments are correct, for this we have the description of the tool: \n
    """
    {description}
    """
    \n
    NOTE: The arguments in the descriptions ARE JUST EXAMPLES FOR THE STRUCTURE. You will need to infer the correct arguments by the last instruction here:

    And this is the last instruction that was given to the agent:
    """
    {last_message}
    """
    The arguments may already be correct! But if they are not, infer the correct arguments for this tool call by the EXAMPLES in the description and the instruction.
    Return ONLY the JSON object of arguments for this tool. Do not include any extra text.

    THE CONTENTS OF THE ARGUMENT SHOULD BE INCLUDED IN THE DESCRIPTION AND NOT MADE UP, AND NEITHER TAKEN FROM THE EXAMPLES.